{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9ce57f-e7e9-4ca9-b6b7-215e1f2f2bae",
   "metadata": {},
   "source": [
    "#Â Intensive calculations\n",
    "\n",
    "Goal:\n",
    "\n",
    "    Use a HPC cluster to run SMRT in //\n",
    "\n",
    "Learning:\n",
    "\n",
    "This tutorial will help you use the following modules\n",
    "\n",
    "    dask_runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfca7f-63fa-4dd5-9e3c-d33c9fd52953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from smrt import make_snowpack, make_ice_column, make_model, make_interface, sensor_list\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644de98-eef8-4d5e-aefc-57aedbc1fb24",
   "metadata": {},
   "source": [
    "We create large snowpack to evaluate the computational cost of snowpacks with many layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77d4c4-472c-44c5-8c7d-894a4eb10e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snowpack(nlayer):\n",
    "    \n",
    "    sp = make_snowpack([0.1] * (nlayer - 1) + [1000], \"exponential\",\n",
    "                   density=np.maximum(200, np.random.normal(350, 50, nlayer)), \n",
    "                   corr_length=np.maximum(50e-6, np.random.normal(500e-6, 200e-6, nlayer)),\n",
    "                   temperature=250)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bd9bf-1b42-45b8-9e12-1d9d75418bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = sensor_list.amsre('37V')\n",
    "m = make_model(\"iba\", \"dort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2db8be-933d-47a7-b413-17a1381bb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "computations = []\n",
    "sps = []\n",
    "\n",
    "for n in list(range(50, 300, 30)) + list(range(300, 1000, 100)):\n",
    "    print(\"nlayer:\", n)\n",
    "    sp = create_snowpack(n)\n",
    "    sps.append(sp)\n",
    "    t0 = time.time()\n",
    "    # m.run(sensor, sp)  # <-- uncomment this !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    t1 = time.time()\n",
    "    computations.append({'nlayer': n, 'time': t1 - t0})\n",
    "\n",
    "computations = pd.DataFrame(computations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5b7d5-309f-4277-b709-a50648bfb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(computations.nlayer, computations.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a5a88-666e-499d-8db8-1bd25d3b65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "m.run(sensor, sps)\n",
    "t1 = time.time()\n",
    "print(f\"total computation time: {t1-t0} using internal SMRT loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ece843-7b6f-4054-947e-98829dc2e99d",
   "metadata": {},
   "source": [
    "# Parallel computation on your machine\n",
    "\n",
    "the easiest way to accelerate simulations is to use all the cpus and cores on your machine. Just add \"parallel_computation=True\" when running the model.\n",
    "\n",
    "The gain is only for calculations at several frequencies or for many snowpacks. Single snowpack at a single frequency is not accelerated, and indeed will be slower with parallel_computation because in this case SMRT disable multi-threading in LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5743822-38b9-4e99-8107-1cdfc752520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "m.run(sensor, sps, parallel_computation=True)\n",
    "t1 = time.time()\n",
    "print(f\"total computation time: {t1-t0} using internal SMRT loop in //\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3225522-87fc-4b9b-ae9f-ec0d612ce70a",
   "metadata": {},
   "source": [
    "# Parallel computation using DASK on an HPC cluster\n",
    "\n",
    "Dask is a Python module for intensive and high memory computations. It works by running one scheduler and one or many workers on a cluster (or on your local machine for testing). These are just python scripts that are run on the cluster. This set is often called \"a dask cluster\" (=the cluster itself + the running scripts). Then, the smrt simulations are \"pushed\" to the scheduler that distributes the simulations on the workers that execute the job in parallal, and return the results back, to SMRT.\n",
    "\n",
    "SMRT abstract most of the boilerplate code to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78257e3-cf9d-4f2c-944e-ebfbdbec6887",
   "metadata": {},
   "source": [
    "The minimum code using an automatically a dask cluster on your local machine is super simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc173ac-76fd-4cf3-baf7-3fa6bf9a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from smrt.runner.dask_runner import DaskParallelRunner\n",
    "\n",
    "client = Client()\n",
    "runner=DaskParallelRunner(client)\n",
    "\n",
    "t0 = time.time()\n",
    "m.run(sensor, sps, runner=runner)\n",
    "t1 = time.time()\n",
    "print(f\"total computation time: {t1-t0} using DASK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e57860-b21c-4251-9dc2-2ba4021e4973",
   "metadata": {},
   "source": [
    "You can monitor the activity on the cluster (if the simulation is long enough):\n",
    "\n",
    "http://localhost:8787\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05843788-d40a-4011-8583-809061c43f9a",
   "metadata": {},
   "source": [
    "Due to the network communication, it is not usually slower than using parallel_computation=True.\n",
    "The main interest is if you have access to a big cluster somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237f4b9-de94-46db-bce6-27d4f61719e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "url = '127.0.0.1:8799'  # url of your cluster. The easiest way to configure the network is to use ssh tunnel (not the most performant)\n",
    "# e.g. ssh -N -f HPCCluster -L8799:localhost:8786 sleep 60\n",
    "\n",
    "\n",
    "client = Client(url, set_as_default=False, direct_to_workers=False)\n",
    "runner=DaskParallelRunner(client)\n",
    "\n",
    "t0 = time.time()\n",
    "m.run(sensor, sps, runner=runner)\n",
    "t1 = time.time()\n",
    "print(f\"total computation time: {t1-t0} using DASK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e21181-4642-4f16-aeed-f69f1a5067f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smrt)",
   "language": "python",
   "name": "smrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
